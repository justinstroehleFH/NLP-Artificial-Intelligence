{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6548f38e",
   "metadata": {},
   "source": [
    "### Template for NLP project\n",
    "\n",
    "The aim of the project is to achieve the following:\n",
    " - Train a neural network that is **at least better than random guessing** on your dataset. The template contains the IMDB dataset for sentiment analysis, however, you can choose any other language related data set with the appropriate NLP task.\n",
    " - Investigate different neural network architectures (different hyperparameters, different layers, different pre-processing). Explain in the presentation, why the final network was selected! **Do not rely on black-box mechanisms.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c548b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.datasets import imdb\n",
    "#from tensorflow.keras.preprocessing import sequence\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LayerNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# if you have installed a different version, replace 'r2.6'  with your version in links provided below\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b175c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n",
      "---review---\n",
      "[1, 307, 5, 1301, 20, 1026, 2511, 87, 2775, 52, 116, 5, 31, 7, 4, 91, 1220, 102, 13, 28, 110, 11, 6, 137, 13, 115, 219, 141, 35, 221, 956, 54, 13, 16, 11, 2714, 61, 322, 423, 12, 38, 76, 59, 1803, 72, 8, 2, 23, 5, 967, 12, 38, 85, 62, 358, 99]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "['the', 'version', 'to', 'date', 'on', 'list', 'draw', 'him', 'critical', 'very', 'love', 'to', 'by', 'br', 'of', 'its', 'tony', 'characters', 'was', 'one', 'life', 'this', 'is', 'go', 'was', 'best', 'least', 'should', 'so', 'done', 'result', 'no', 'was', 'with', 'this', 'understood', 'only', 'war', \"couldn't\", 'that', 'her', 'get', 'would', 'johnny', 'we', 'in', 'and', 'are', 'to', 'business', 'that', 'her', 'because', 'story', 'use', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# load imdb dataset\n",
    "# links to dataset\n",
    "# original dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# version in tensorflow: https://www.tensorflow.org/versions/r2.6/api_docs/python/tf/keras/datasets/imdb\n",
    "\n",
    "# select your vocabulary size\n",
    "vocabularySize = 5000\n",
    "# load data (it is already pre-processed)\n",
    "# optional: add other pre.processing steps like stopword removal\n",
    "(xTrain, yTrain), (xTest, yTest) = tf.keras.datasets.imdb.load_data(num_words=vocabularySize)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(xTrain), len(xTest)))\n",
    "\n",
    "# look at the data\n",
    "print('---review---')\n",
    "print(xTrain[123])\n",
    "print('---label---')\n",
    "print(yTrain[123])\n",
    "\n",
    "# look at the respective words\n",
    "word2id = tf.keras.datasets.imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in xTrain[123]])\n",
    "\n",
    "\n",
    "# import nltk  # probably have to install that first\n",
    "# nltk.download('stopwords') # this might take somne time\n",
    "# from nltk.corpus import stopwords\n",
    "#\n",
    "# stopWords = set(stopwords.words('english'))\n",
    "# stopWords.add(\"br\") #added br tag to stopwords\n",
    "# vocabularySize = vocabularySize-len(stopWords)\n",
    "# print(vocabularySize)\n",
    "# for index in range(len(xTrain)):\n",
    "#     xTrain[index] = [ (w-len(stopWords))%vocabularySize for w in xTrain[index] if id2word.get(w) not in stopWords]\n",
    "# for index in range(len(xTest)):\n",
    "#     xTest[index] = [ (w-len(stopWords))%vocabularySize for w in xTest[index] if id2word.get(w) not in stopWords]\n",
    "# #xTrain[123] = [w for w in xTrain[123] if id2word.get(w) not in stopWords]\n",
    "#\n",
    "# print(xTrain[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f7933da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum train review length: 2494\n",
      "Maximum test review length: 2315\n",
      "Minimum train review length: 11\n",
      "Minimum test review length: 7\n"
     ]
    }
   ],
   "source": [
    "# get properties of the dataset\n",
    "print('Maximum train review length: {}'.format(len(max(xTrain, key=len))))\n",
    "print('Maximum test review length: {}'.format(len(max(xTest, key=len))))\n",
    "print('Minimum train review length: {}'.format(len(min(xTrain, key=len))))\n",
    "print('Minimum test review length: {}'.format(len(min(xTest, key=len))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96094e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select maximum number of words as input length\n",
    "# pad or truncated (this is done automatically) your data\n",
    "maxWords = 1000\n",
    "xTrain = tf.keras.preprocessing.sequence.pad_sequences(xTrain, maxlen=maxWords)\n",
    "xTest = tf.keras.preprocessing.sequence.pad_sequences(xTest, maxlen=maxWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c45999cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 1000, 64)          320000    \n",
      "                                                                 \n",
      " bidirectional_30 (Bidirecti  (None, 1000, 8)          2208      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_31 (Bidirecti  (None, 1000, 32)         2496      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " layer_normalization_23 (Lay  (None, 1000, 32)         64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " bidirectional_32 (Bidirecti  (None, 1000, 16)         2624      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_33 (Bidirecti  (None, 1000, 64)         9600      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " layer_normalization_24 (Lay  (None, 1000, 64)         128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " bidirectional_34 (Bidirecti  (None, 1000, 32)         10368     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_35 (Bidirecti  (None, 128)              37632     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " layer_normalization_25 (Lay  (None, 128)              256       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385,505\n",
      "Trainable params: 385,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# setup the neural network architecture\n",
    "# check out the respective tensorflow help page: https://www.tensorflow.org/guide/keras/rnn\n",
    "model=tf.keras.Sequential()\n",
    "\n",
    "def original_code():\n",
    "    # define size of embedding, see https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/Embedding\n",
    "    # optional: use a different embedding like word2vec or other options available within tensorflow\n",
    "    embeddingSize = 128\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    # add recurrent layers:\n",
    "    # e.g. a SimpleRNN (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/SimpleRNN) with\n",
    "    # LayerNormalization (https://www.tensorflow.org/versions/r2.9/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "    model.add(tf.keras.layers.SimpleRNN(100))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt1():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.SimpleRNN(100))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt2():\n",
    "    embeddingSize = 256\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.SimpleRNN(100))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt3():\n",
    "    embeddingSize = 128\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(128))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt4():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(4))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt5():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.GRU(256))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt6():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt7():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt8():\n",
    "    #maxWords = 250\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt9():\n",
    "    #maxWords = 250\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(4)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt10():\n",
    "    #maxWords = 250\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def attempt11():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "def finalAttempt():\n",
    "    embeddingSize = 64\n",
    "    model.add(tf.keras.layers.Embedding(vocabularySize, embeddingSize, input_length=maxWords))\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16 ,return_sequences=True)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32 ,return_sequences=True)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "finalAttempt()\n",
    "\n",
    "# add layer for output\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print model and check number of parameters\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18535444",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  8/390 [..............................] - ETA: 39:48 - loss: 0.8063 - accuracy: 0.5195"
     ]
    }
   ],
   "source": [
    "# set parameters for network training\n",
    "batchSize = 64\n",
    "numEpochs = 5\n",
    "\n",
    "# train your model\n",
    "model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\n",
    "xValid, yValid = xTrain[:batchSize], yTrain[:batchSize]\n",
    "xTrain2, yTrain2 = xTrain[batchSize:], yTrain[batchSize:]\n",
    "hist = model.fit(xTrain2, yTrain2, validation_data=(xValid, yValid), batch_size=batchSize, epochs=numEpochs)\n",
    "\n",
    "# check result\n",
    "scores = model.evaluate(xTest, yTest, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise Model Accuracy\n",
    "plt.plot(hist.history['accuracy'])\n",
    "\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.yticks(np.arange(0, 1, step=0.1))\n",
    "plt.xticks(np.arange(0, 5, step=1))\n",
    "plt.grid()\n",
    "\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualise Loss\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.xticks(np.arange(0, 5, step=1))\n",
    "plt.grid()\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}